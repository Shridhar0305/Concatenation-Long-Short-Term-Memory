{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "prerequisite-field",
   "metadata": {
    "tags": []
   },
   "source": [
    "# IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sporting-guitar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7efcfbd495b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import *\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import config2_ctlstm_modified_v2 as config\n",
    "import MODEL\n",
    "import UTILS\n",
    "\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-cleaning",
   "metadata": {
    "tags": []
   },
   "source": [
    "# HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "passing-demand",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\n",
    "    description='arguments')\n",
    "parser.add_argument('--init', type=int, default=0, help='init number')\n",
    "parser.add_argument('--fold', type=int, default=0, help='fold number')\n",
    "parser.add_argument('--model_name', type=str, default='ctlstm', help='model_name')\n",
    "parser.add_argument('--date', type=str, default='20240804', help='date')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "determined-designation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P_ERA' 'Lai' 'VPD_ERA' 'TA_ERA' 'SW_IN_ERA' 'GPP_NT_VUT_REF' 'RECO'\n",
      " 'pft_MF' 'pft_CRO' 'pft_CSH' 'pft_DBF' 'pft_EBF' 'pft_ENF' 'pft_GRA'\n",
      " 'pft_OSH' 'pft_SAV' 'pft_SNO' 'pft_WET' 'pft_WSA' 'climate_Arctic'\n",
      " 'climate_Continental' 'climate_Temperate' 'climate_Tropical'\n",
      " 'climate_Arid' 'Lat' 'Lon']\n",
      "Hyperparameters:ctlstm\n",
      "window : 30\n",
      "stride : 15\n",
      "dynamic_channels : [0, 1, 2, 3, 4]\n",
      "static_channels : [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "output_channels : [6]\n",
      "unknown : nan\n",
      "model_name : ctlstm\n",
      "forward_code_dim : 256\n",
      "device : cuda\n",
      "dropout : 0.4\n",
      "train : True\n",
      "batch_size : 64\n",
      "epochs : 1\n",
      "learning_rate : 0.001\n",
      "init : 1\n",
      "fold : 0\n"
     ]
    }
   ],
   "source": [
    "# TIME SERIES INFO\n",
    "window = config.window\n",
    "stride = config.stride\n",
    "channels = config.channels_names\n",
    "print(channels)\n",
    "# CHANNELS INFO\n",
    "dynamic_channels = config.dynamic_channels\n",
    "static_channels = config.static_channels\n",
    "output_channels = config.output_channels\n",
    "# metaflux_channels = config.metaflux_channels\n",
    "no_normalize_channels = config.no_normalize_channels\n",
    "normalize_channels = config.normalize_channels\n",
    "\n",
    "# LABELS INFO\n",
    "unknown = config.unknown\n",
    "\n",
    "# MODEL INFO\n",
    "model_name = args.model_name\n",
    "forward_code_dim = config.forward_code_dim\n",
    "device = torch.device(config.device)\n",
    "dropout = config.dropout\n",
    "\n",
    "# TRAIN INFO\n",
    "train = config.train\n",
    "batch_size = config.batch_size\n",
    "epochs = config.epochs\n",
    "learning_rate = config.learning_rate\n",
    "init = args.init\n",
    "fold = args.fold\n",
    "\n",
    "print(\"Hyperparameters:{}\".format(model_name))\n",
    "print(\"window : {}\".format(window))\n",
    "print(\"stride : {}\".format(stride))\n",
    "print(\"dynamic_channels : {}\".format(dynamic_channels))\n",
    "print(\"static_channels : {}\".format(static_channels))\n",
    "print(\"output_channels : {}\".format(output_channels))\n",
    "print(\"unknown : {}\".format(unknown))\n",
    "print(\"model_name : {}\".format(model_name))\n",
    "print(\"forward_code_dim : {}\".format(forward_code_dim))\n",
    "print(\"device : {}\".format(device))\n",
    "print(\"dropout : {}\".format(dropout))\n",
    "print(\"train : {}\".format(train))\n",
    "print(\"batch_size : {}\".format(batch_size))\n",
    "print(\"epochs : {}\".format(epochs))\n",
    "print(\"learning_rate : {}\".format(learning_rate))\n",
    "print(\"init : {}\".format(init))\n",
    "print(\"fold : {}\".format(fold))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charitable-purchase",
   "metadata": {},
   "source": [
    "# DEFINE DIRECTORIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "blank-brick",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATE = args.date\n",
    "PREPROCESSED_DIR = config.PREPROCESSED_DIR\n",
    "RESULT_DIR = os.path.join(config.RESULT_DIR, DATE)\n",
    "MODEL_DIR = os.path.join(config.MODEL_DIR, DATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-holder",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "romance-twelve",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file):\n",
    "    dataset = np.load(os.path.join(PREPROCESSED_DIR, \"{}.npz\".format(file)), allow_pickle=True)\n",
    "    return dataset\n",
    "\n",
    "def get_data(dataset,preprocessed=True):\n",
    "    data = dataset[\"data\"]\n",
    "    print(data.shape)\n",
    "    if preprocessed:\n",
    "        data_mean = dataset[\"train_data_means\"]\n",
    "        data_std =  dataset[\"train_data_stds\"]\n",
    "\n",
    "        #print(data_mean.shape)\n",
    "        normalized_data = np.zeros_like(data)\n",
    "        if len(data.shape)==4:\n",
    "            for feature in range(data_mean.shape[0]):\n",
    "                if data_std[feature]!=0:\n",
    "                    normalized_data[:,:,:,feature] = (data[:,:,:,feature] - data_mean[feature])/data_std[feature]\n",
    "                else:\n",
    "                    normalized_data[:,:,:,feature] = data[:,:,:,feature]\n",
    "                normalized_data[:,:,:,no_normalize_channels] = data[:,:,:,no_normalize_channels]\n",
    "        else:\n",
    "            normalized_data[:,:,normalize_channels] = data[:,:,normalize_channels] - data_mean[normalize_channels]/data_std[normalize_channels]\n",
    "            # normalized_data[:,:,-1] = data[:,:,-1]\n",
    "        data = normalized_data\n",
    "    data = np.nan_to_num(data, nan=unknown)\n",
    "    #print(data.shape)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "respiratory-compilation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_test(dataset, preprocessed=True):\n",
    "    data = dataset[\"data\"]\n",
    "    data_final = []\n",
    "    if preprocessed:\n",
    "        data_mean = dataset[\"train_data_means\"]\n",
    "        data_std = dataset[\"train_data_stds\"]\n",
    "        for data_value in data:\n",
    "            normalized_data = np.zeros_like(data_value)\n",
    "            if len(data_value.shape)==3:\n",
    "                for feature in range(data_mean.shape[0]):\n",
    "                    if data_std[feature]!=0:\n",
    "                        normalized_data[:,:,feature] = (data_value[:,:,feature] - data_mean[feature])/data_std[feature]\n",
    "                    else:\n",
    "                        normalized_data[:,:,feature] = data_value[:,:,feature]\n",
    "                normalized_data[:,:,no_normalize_channels] = data_value[:,:,no_normalize_channels]\n",
    "            else:\n",
    "                 normalized_data[:,normalize_channels] = data_value[:,normalize_channels] - data_mean[normalize_channels]/data_std[normalize_channels]\n",
    "            data_final.append(normalized_data)\n",
    "            \n",
    "    data_final = np.array(data_final, dtype=object)\n",
    "    data_final = np.nan_to_num(data_final, nan=unknown)\n",
    "    return data_final\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-effort",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def unstride_array_list(strided_data_list):\n",
    "#     unstrided_list = []\n",
    "\n",
    "#     for strided_data in strided_data_list:\n",
    "#         shape = strided_data.shape\n",
    "#         data = config.unknown * np.ones((shape[0], 1 + (shape[1] // config.stride), shape[2]))\n",
    "#         data[:, config.stride:] = strided_data[:, ::2, config.stride:]\n",
    "#         first_part = config.unknown * np.ones((shape[0], 1, config.stride))\n",
    "#         second_part = strided_data[:, 1::2, config.stride + 1:]\n",
    "#         data[:, :config.stride] = np.concatenate((first_part, second_part), axis=1)\n",
    "#         data = np.reshape(data, (shape[0], -1))\n",
    "#         unstrided_list.append(data)\n",
    "\n",
    "#     return unstrided_list\n",
    "\n",
    "\n",
    "def unstride_array(strided_data_list):\n",
    "    combined_second_parts = []\n",
    "    for strided_data in strided_data_list:\n",
    "        shape = strided_data.shape\n",
    "        if shape[1] > 0:\n",
    "            second_part = strided_data[:, 1::2, config.stride + 1:]\n",
    "        else:\n",
    "            second_part = np.empty((shape[0], 0, shape[2]))\n",
    "        combined_second_parts.append(second_part)\n",
    "\n",
    "    combined_unstrided_data = np.concatenate(combined_second_parts, axis=1)\n",
    "    return combined_unstrided_data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "behind-carbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_data_test(dataset, preprocessed=True, unknown=0):\n",
    "#     data = dataset[\"data\"]\n",
    "#     data_final = np.empty_like(data)  # Initialize with the same shape as data\n",
    "    \n",
    "#     if preprocessed:\n",
    "#         data_mean = dataset[\"train_data_means\"]\n",
    "#         data_std = dataset[\"train_data_stds\"]\n",
    "#         no_normalize_channels = []  # Assuming this is defined somewhere\n",
    "        \n",
    "#         for idx, data_value in enumerate(data):\n",
    "#             normalized_data = np.zeros_like(data_value)\n",
    "            \n",
    "#             if len(data_value.shape) == 3:\n",
    "#                 for feature in range(data_mean.shape[0]):\n",
    "#                     if data_std[feature] != 0:\n",
    "#                         # Normalize only the second half along the first axis\n",
    "#                         normalized_data[len(data_value)//2:, :, feature] = (\n",
    "#                             (data_value[len(data_value)//2:, :, feature] - data_mean[feature]) / data_std[feature]\n",
    "#                         )\n",
    "#                         # Copy channels that should not be normalized\n",
    "#                         for channel in no_normalize_channels:\n",
    "#                             normalized_data[:, :, channel] = data_value[:, :, channel]\n",
    "#                     else:\n",
    "#                         normalized_data[:, :, feature] = data_value[:, :, feature]\n",
    "                        \n",
    "#             else:  # Assuming 2D case\n",
    "#                 normalize_channels = []  # Assuming this is defined somewhere\n",
    "#                 for channel in normalize_channels:\n",
    "#                     if data_std[channel] != 0:\n",
    "#                         # Normalize only the second half along the first axis\n",
    "#                         normalized_data[len(data_value)//2:, channel] = (\n",
    "#                             (data_value[len(data_value)//2:, channel] - data_mean[channel]) / data_std[channel]\n",
    "#                         )\n",
    "#                     else:\n",
    "#                         normalized_data[:, channel] = data_value[:, channel]\n",
    "            \n",
    "#             data_final[idx] = normalized_data\n",
    "    \n",
    "#     data_final = np.nan_to_num(data_final, nan=unknown)\n",
    "#     return data_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "chief-tractor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(145, 533, 30, 26)\n",
      "(145, 533, 30, 26)\n",
      "145 533 30 26\n"
     ]
    }
   ],
   "source": [
    "file, index = \"strided_train\", \"in_indices\"\n",
    "dataset = load_dataset(file)\n",
    "data = dataset[\"data\"]\n",
    "normalized_data = np.zeros_like(data)\n",
    "print(normalized_data.shape)\n",
    "data = get_data(dataset)\n",
    "nodes, years, window, channels = data.shape\n",
    "print(nodes, years, window, channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "integral-azerbaijan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file, index = \"strided_test\", \"in_indices\"\n",
    "# dataset = load_dataset(file)\n",
    "# data = dataset[\"data\"]\n",
    "# normalized_data = np.zeros_like(data)\n",
    "# print(normalized_data.shape)\n",
    "# for i in data:\n",
    "#     print(i.shape)\n",
    "# print(data.shape)\n",
    "# data = get_data(dataset)\n",
    "# nodes, years, window, channels = data.shape\n",
    "# print(nodes, years, window, channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceramic-opera",
   "metadata": {},
   "source": [
    "# TRAIN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "graduate-yorkshire",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Parameters:289025\n",
      "ctlstm(\n",
      "  (encoder): LSTM(24, 256, batch_first=True)\n",
      "  (out): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      ")\n",
      "(145, 533, 30, 26)\n",
      "145 533 30 26\n",
      "Epoch:1\tTrain Loss:0.3729\tVal Loss:0.3785\tMin Loss:10000.0000\tTime:6.4336\n"
     ]
    }
   ],
   "source": [
    "if train:\n",
    "#     print(\"fold:{}\\tinit:{}\".format(fold, init))\n",
    "\n",
    "    # BUILD MODEL\n",
    "    model = getattr(MODEL, \"ctlstm\")(input_dynamic_channels=len(dynamic_channels), input_static_channels=len(static_channels), hidden_dim=forward_code_dim, output_channels=len(output_channels), dropout=dropout)\n",
    "    model = model.to(device)\n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(\"#Parameters:{}\".format(pytorch_total_params))\n",
    "    print(model)\n",
    "    criterion = torch.nn.MSELoss(reduction=\"none\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    min_loss = 10000\n",
    "\n",
    "    for epoch in range(1,epochs+1):\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        # LOSS ON TRAIN SET\n",
    "        model.train()\n",
    "\n",
    "        # LOAD DATA\n",
    "        file, index = \"strided_train\", \"in_indices\"\n",
    "        dataset = load_dataset(file)\n",
    "        data = get_data(dataset)\n",
    "        nodes, years, window, channels = data.shape\n",
    "        print(nodes, years, window, channels)\n",
    "\n",
    "        # GET RANDOM YEARS\n",
    "        random_years = np.zeros((nodes, years))\n",
    "        for node in range(nodes):\n",
    "            random_years[node] = random.sample(range(years), years)\n",
    "        random_years = random_years.astype(np.int64)\n",
    "        # print(random_years.shape)\n",
    "\n",
    "        # LOSS\n",
    "        epoch_loss = 0\n",
    "        for year in range(random_years.shape[1]):\n",
    "\n",
    "            #Get instance for each node\n",
    "            node_data = data[np.arange(nodes), random_years[:, year]]\n",
    "            # print(node_data.shape)\n",
    "\n",
    "            random_batches = random.sample(range(node_data.shape[0]),node_data.shape[0])\n",
    "            for batch in range(math.ceil(nodes/batch_size)):\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # GET BATCH DATA AND LABEL\n",
    "                random_batch = random_batches[batch*batch_size:(batch+1)*batch_size]\n",
    "                batch_data = torch.from_numpy(node_data[random_batch]).float().to(device)\n",
    "                batch_dynamic_input = batch_data[:, :, dynamic_channels].float().to(device)\n",
    "                batch_static_input = batch_data[:, :, static_channels].float().to(device)\n",
    "#                 print(batch_dynamic_input.shape, \"batch dybamic inout\")\n",
    "#                 print(batch_static_input.shape, \"batch static inptu\")\n",
    "#                 print(batch_data.shape)\n",
    "#                 print(output_channels)\n",
    "#                 print(batch_data[:, :, output_channels])\n",
    "                batch_label = batch_data[:, :, output_channels].float().to(device)\n",
    "                # print(batch_dynamic_input.shape, batch_static_input.shape, batch_label.shape)\n",
    "\n",
    "                # GET OUTPUT\n",
    "                batch_pred = model(x_dynamic=batch_dynamic_input, x_static=batch_static_input)\n",
    "#                 print(batch_pred.shape, \"This is batch pred shape\")\n",
    "\n",
    "                # CALCULATE LOSS\n",
    "                batch_loss = criterion(batch_label, batch_pred)\t\t\t\t\t\t\t\t\t\t\t# PER CHANNEL LOSS\n",
    "                mask = (batch_label!=unknown).float()\t\t\t\t\t\t\t\t\t\t\t\t\t# CREATE MASK\n",
    "                batch_loss = batch_loss * mask\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# MULTIPLY MASK\n",
    "                batch_loss, mask = torch.sum(batch_loss, dim=2), (torch.sum(mask, dim=2)>0).float()\t\t# PER INSTANCE LOSS\n",
    "                batch_loss = torch.sum(batch_loss)/torch.sum(mask)\t\t\t\t\t\t\t\t\t\t# MEAN SEQUENCE LOSS\n",
    "                # print(batch_loss.shape)\n",
    "\n",
    "                # LOSS BACKPROPOGATE\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # AGGREGATE LOSS\n",
    "                epoch_loss += batch_loss.item()\n",
    "\n",
    "        epoch_loss /= ((batch+1)*(year+1))\n",
    "        print('Epoch:{}\\tTrain Loss:{:.4f}'.format(epoch, epoch_loss), end=\"\\t\")\n",
    "        train_loss.append(epoch_loss)\n",
    "\n",
    "        # SCORE ON VALIDATION SET\n",
    "        model.eval()\n",
    "\n",
    "        # LOAD DATA\n",
    "        data = data[:,int(0.8*years):,:,:]      \n",
    "        nodes, years, window, channels = data.shape\n",
    "        \n",
    "#         file, index = \"strided_valid\", \"in_indices\"\n",
    "#         dataset = load_dataset(file)\n",
    "#         data = get_data(dataset)\n",
    "#         nodes, years, window, channels = data.shape\n",
    "        # print(nodes, years, window, channels)\n",
    "\n",
    "        # SCORE\n",
    "        epoch_loss = 0\n",
    "        for year in range(years):\n",
    "\n",
    "            #Get instance for each node\n",
    "            node_data = data[np.arange(nodes), year]\n",
    "            # print(node_data.shape)\n",
    "\n",
    "            for batch in range(math.ceil(nodes/batch_size)):\n",
    "\n",
    "                # GET BATCH DATA AND LABEL\n",
    "                batch_data = torch.from_numpy(node_data[batch*batch_size:(batch+1)*batch_size]).float().to(device)\n",
    "                batch_dynamic_input = batch_data[:, :, dynamic_channels].float().to(device)\n",
    "                batch_static_input = batch_data[:, :, static_channels].float().to(device)\n",
    "                batch_label = batch_data[:, :, output_channels].float().to(device)\n",
    "                # print(batch_dynamic_input.shape, batch_static_input.shape, batch_label.shape)\n",
    "\n",
    "                # GET OUTPUTbatch static inptu\n",
    "                batch_pred = model(x_dynamic=batch_dynamic_input, x_static=batch_static_input)\n",
    "                # print(batch_pred.shape)\n",
    "\n",
    "                # CALCULATE LOSS\n",
    "                batch_loss = criterion(batch_label, batch_pred)\t\t\t\t\t\t\t\t\t\t\t# PER CHANNEL LOSS\n",
    "                mask = (batch_label!=unknown).float()\t\t\t\t\t\t\t\t\t\t\t\t\t# CREATE MASK\n",
    "                batch_loss = batch_loss * mask\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# MULTIPLY MASK\n",
    "                batch_loss, mask = torch.sum(batch_loss, dim=2), (torch.sum(mask, dim=2)>0).float()\t\t# PER SEQUENCE LOSS\n",
    "                batch_loss = torch.sum(batch_loss)/torch.sum(mask)\t\t\t\t\t\t\t\t\t\t# MEAN SEQUENCE LOSS\n",
    "                # print(batch_loss.shape)\n",
    "\n",
    "                # AGGREGATE LOSSbut we need fp\n",
    "                epoch_loss += batch_loss.item()\n",
    "\n",
    "        epoch_loss /= ((batch+1)*(year+1))\n",
    "        print(\"Val Loss:{:.4f}\\tMin Loss:{:.4f}\".format(epoch_loss, min_loss), end=\"\\t\")\n",
    "        valid_loss.append(epoch_loss)\n",
    "        if min_loss>epoch_loss:\n",
    "            min_loss = epoch_loss\n",
    "            torch.save(model.state_dict(), os.path.join(MODEL_DIR, \"{}\".format(model_name)))\n",
    "        end = time.time()\n",
    "        print(\"Time:{:.4f}\".format(end-start))\n",
    "\n",
    "    # PLOT LOSS\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    ax1.set_xlabel(\"#Epoch\", fontsize=50)\n",
    "\n",
    "    # PLOT TRAIN LOSS\n",
    "    lns1 = ax1.plot(train_loss, color='red', marker='o', linewidth=4, label=\"TRAIN LOSS\")\n",
    "\n",
    "    # PLOT VALIDATION SCORE\n",
    "    ax2 = ax1.twinx()\n",
    "    lns2 = ax2.plot(valid_loss, color='blue', marker='o', linewidth=4, label=\"VAL LOSS\")\n",
    "\n",
    "    # added these three lines\n",
    "    lns = lns1+lns2\n",
    "    labs = [l.get_label() for l in lns]\n",
    "    ax1.legend(lns, labs, loc=\"upper right\", fontsize=40, frameon=False)\n",
    "\n",
    "    plt.tight_layout(pad=0.0,h_pad=0.0,w_pad=0.0)\n",
    "    plt.savefig(os.path.join(RESULT_DIR, \"{}_SCORE.pdf\".format(model_name)), format = \"pdf\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-economics",
   "metadata": {},
   "source": [
    "# TEST MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-surprise",
   "metadata": {},
   "source": [
    "## IN DISTRIBUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "according-aggregate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape1\n",
      "Per Sample RMSE:18.9865\tPer Node RMSE:18.9865\tPer Sample R2:-0.1181\tPer Node R2:-0.1181\n"
     ]
    }
   ],
   "source": [
    "# print(\"IN\\tfold:{}\\tinit:{}\".format(fold, init))\n",
    "\n",
    "# BUILD MODEL\n",
    "model = getattr(MODEL, \"ctlstm\")(input_dynamic_channels=len(dynamic_channels), input_static_channels=len(static_channels), hidden_dim=forward_code_dim, output_channels=len(output_channels), dropout=dropout)\n",
    "model = model.to(device)\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "# print(\"#Parameters:{}\".format(pytorch_total_params))\n",
    "# print(model)\n",
    "\n",
    "# LOAD MODEL\n",
    "model.load_state_dict(torch.load(os.path.join(MODEL_DIR, \"{}\".format(model_name))))\n",
    "model.eval()\n",
    "\n",
    "# LOAD DATA\n",
    "file, index = \"strided_test\", \"in_indices\"\n",
    "dataset = load_dataset(file)\n",
    "#do a for loop for each test site: 1, x, 30, 26 where x is the sliding windows\n",
    "data = get_data_test(dataset)\n",
    "# print(data.shape)\n",
    "dataset_true_final = []\n",
    "dataset_pred_final = []\n",
    "# dataset_metaflux_final = []\n",
    "\n",
    "dataset_true_final_data = []\n",
    "dataset_pred_final_data = []\n",
    "# dataset_metaflux_final_data = []\n",
    "for data_value in data:\n",
    "#     print(data_value.shape)\n",
    "    data_value = np.expand_dims(data_value, axis=0)\n",
    "#     print(data_value.shape)\n",
    "    nodes, years, window, channels = data_value.shape\n",
    "#     print(nodes, years, window, channels)\n",
    "\n",
    "    dataset_true = unknown*np.ones((nodes, years, stride, len(output_channels)), dtype=np.float32)\n",
    "#     print(\"this is the dtat true shape\")\n",
    "#     print(dataset_true.shape)\n",
    "    dataset_pred = unknown*np.ones((nodes, years, stride, len(output_channels)), dtype=np.float32)\n",
    "#     dataset_metaflux = unknown*np.ones((nodes, years, stride, len(metaflux_channels)), dtype=np.float32)\n",
    "    for year in range(years):\n",
    "\n",
    "        #Get instance for each node\n",
    "        node_data = data_value[np.arange(nodes), year]\n",
    "        node_data = np.array(node_data, dtype=np.float32)\n",
    "#         print(\"this is node data shape\")\n",
    "#         print(node_data.shape)\n",
    "\n",
    "        for batch in range(math.ceil(nodes/batch_size)):\n",
    "\n",
    "            # GET BATCH DATA AND LABEL\n",
    "            batch_data = torch.from_numpy(node_data[batch*batch_size:(batch+1)*batch_size]).float().to(device)\n",
    "            batch_dynamic_input = batch_data[:, :, dynamic_channels].float().to(device)\n",
    "            batch_static_input = batch_data[:, :, static_channels].float().to(device)\n",
    "            batch_label = batch_data[:, :, output_channels].float().to(device)   #this is the true value\n",
    "#             batch_metaflux = batch_data[:, :, metaflux_channels].float().to(device)\n",
    "#             print(batch_dynamic_input.shape, batch_static_input.shape, batch_label.shape)\n",
    "            \n",
    "        \n",
    "            # GET OUTPUT\n",
    "            batch_pred = model(x_dynamic=batch_dynamic_input, x_static=batch_static_input)    #this is the pred value \n",
    "            print(\"this is batch pred shape\")\n",
    "            print(batch_pred.shape)\n",
    "            \n",
    "            #Slicing the batch_pred and batch_label into two halves and taking the second half.\n",
    "            half_idx = stride\n",
    "            batch_pred_second_half = batch_pred[:, -half_idx:]\n",
    "            batch_label_second_half = batch_label[:, -half_idx:]\n",
    "#             batch_metaflux_second_half = batch_metaflux[:, -half_idx:]\n",
    "#             print(\"this is the shape of the half one\")\n",
    "#             print(batch_label_second_half.shape)\n",
    "\n",
    "            # STORE OUTPUT\n",
    "            dataset_true[batch*batch_size:(batch+1)*batch_size, year] = batch_label_second_half.detach().cpu().numpy()  #this is the true value\n",
    "            dataset_pred[batch*batch_size:(batch+1)*batch_size, year] = batch_pred_second_half.detach().cpu().numpy()    #this is the predicted value\n",
    "#             dataset_metaflux[batch*batch_size:(batch+1)*batch_size, year] = batch_metaflux_second_half.detach().cpu().numpy()\n",
    "#     print(\"dtaset true\")\n",
    "#     print(dataset_true.shape)\n",
    "#     print(\"dtaset pred\")\n",
    "#     print(dataset_pred.shape)\n",
    "    dataset_true_final.append(dataset_true)\n",
    "    dataset_pred_final.append(dataset_pred)\n",
    "    \n",
    "# print(\"thos one\")\n",
    "# print(dataset_true_final[0].shape)\n",
    "# print(dataset_true_final[1].shape)\n",
    "#dataset_true_final = np.concatenate(dataset_true_final, axis=1)\n",
    "# print(\"THIS IS THE SHAPE OF THE DATASET TRUE FINAL\")\n",
    "# print(dataset_true_final.shape)\n",
    "\n",
    "#dataset_pred_final = np.concatenate(dataset_pred_final, axis=1)\n",
    "# print(\"THIS IS THE SHAPE OF THE DATASET PRED FINAL\")\n",
    "# print(dataset_pred_final.shape)\n",
    "\n",
    "# for true_final in dataset_true_final:\n",
    "#     dataset_true_final_data.append((true_final*dataset[\"train_data_stds\"][output_channels])+dataset[\"train_data_means\"][output_channels])\n",
    "    \n",
    "# for pred_final in dataset_pred_final:\n",
    "#     dataset_pred_final_data.append((pred_final*dataset[\"train_data_stds\"][output_channels])+dataset[\"train_data_means\"][output_channels])\n",
    "    \n",
    "# for metaflux_final in dataset_metaflux_final:\n",
    "#     dataset_metaflux_final_data.append((metaflux_final*dataset[\"train_data_stds\"][metaflux_channels])+dataset[\"train_data_means\"][metaflux_channels])\n",
    "# dataset_pred_final_data = np.array(dataset_pred_final_data)\n",
    "# print(\"shape1\")\n",
    "# print(dataset_pred_final_data.shape)\n",
    "# print(dataset_true_final_data.shape)\n",
    "# print(\"shape2\")\n",
    "# # print((UTILS.unstride_array(dataset_true_final)).shape)\n",
    "\n",
    "# print(dataset_true_final.shape)\n",
    "# print(dataset_pred_final.shape)\n",
    "\n",
    "# for value in dataset_true_final_data:\n",
    "#     print(\"one\")\n",
    "#     print(value.shape)\n",
    "#     dataset_true_final = unstride_array(value)\n",
    "#     print(\"two\")\n",
    "#     print(dataset_true_final.shape)\n",
    "final_output_true = []\n",
    "for value in dataset_true_final:\n",
    "#     print(value.shape)\n",
    "    value = np.reshape(value, (value.shape[0], value.shape[1] * value.shape[2], value.shape[3]))\n",
    "    print(value.shape)\n",
    "    final_output_true.append(value)\n",
    "    \n",
    "final_output_pred = []\n",
    "for value in dataset_pred_final:\n",
    "#     print(value.shape)\n",
    "    value = np.reshape(value, (value.shape[0], value.shape[1] * value.shape[2], value.shape[3]))\n",
    "    print(value.shape)\n",
    "    final_output_pred.append(value)\n",
    "    \n",
    "\n",
    "# per_sample_RMSE = UTILS.per_sample_RMSE(dataset_true_final, dataset_pred_final, unknown)\n",
    "# _, per_node_RMSE = UTILS.per_node_RMSE(dataset_true_final, dataset_pred_final, unknown)\n",
    "# per_sample_R2 = UTILS.per_sample_R2(dataset_true_final, dataset_pred_final, unknown)\n",
    "# _, per_node_R2 = UTILS.per_node_R2(dataset_true_final, dataset_pred_final, unknown)\n",
    "# print(\"Per Sample RMSE:{:.4f}\\tPer Node RMSE:{:.4f}\\tPer Sample R2:{:.4f}\\tPer Node R2:{:.4f}\".format(per_sample_RMSE, per_node_RMSE, per_sample_R2, per_node_R2))\n",
    "# with open(os.path.join(RESULT_DIR, \"{}_{}_{}\".format(file, index, \"ameriflux_true_{}\".format(fold))), 'wb') as f:\n",
    "#     pickle.dump(final_output_true, f)\n",
    "# with open(os.path.join(RESULT_DIR, \"{}_{}_{}_{}\".format(file, index, \"ameriflux_pred\", model_name)), 'wb') as f:\n",
    "#     pickle.dump(final_output_pred, f)\n",
    "    \n",
    "with open(os.path.join(RESULT_DIR, \"{}_{}_{}\".format(file, index, \"true_{}\".format(fold))), 'wb') as f:\n",
    "    pickle.dump(final_output_true, f)\n",
    "with open(os.path.join(RESULT_DIR, \"{}_{}_{}\".format(file, index, model_name)), 'wb') as f:\n",
    "    pickle.dump(final_output_pred, f)\n",
    "\n",
    "# np.save(os.path.join(RESULT_DIR, \"{}_{}_{}\".format(file, index, \"ameriflux_true_{}\".format(fold))), dataset_true_final)\n",
    "# np.save(os.path.join(RESULT_DIR, \"{}_{}_{}_{}\".format(file, index, \"ameriflux_pred\", model_name)), dataset_pred_final)\n",
    "# np.save(os.path.join(RESULT_DIR, \"{}_{}_{}\".format(file, index, \"metaflux_{}\".format(fold))), dataset_metaflux_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-ready",
   "metadata": {},
   "source": [
    "## OUT DISTRIBUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggressive-construction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"IN\\tfold:{}\\tinit:{}\".format(fold, init))\n",
    "\n",
    "# BUILD MODEL\n",
    "model = getattr(MODEL, \"ctlstm\")(input_dynamic_channels=len(dynamic_channels), input_static_channels=len(static_channels), hidden_dim=forward_code_dim, output_channels=len(output_channels), dropout=dropout)\n",
    "model = model.to(device)\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "# print(\"#Parameters:{}\".format(pytorch_total_params))\n",
    "# print(model)\n",
    "\n",
    "# LOAD MODEL\n",
    "model.load_state_dict(torch.load(os.path.join(MODEL_DIR, \"{}\".format(model_name))))\n",
    "model.eval()\n",
    "\n",
    "# LOAD DATA\n",
    "file, index = \"strided_test\", \"out_indices\"\n",
    "dataset = load_dataset(file)\n",
    "#do a for loop for each test site: 1, x, 30, 26 where x is the sliding windows\n",
    "data = get_data_test(dataset)\n",
    "# print(data.shape)\n",
    "dataset_true_final = []\n",
    "dataset_pred_final = []\n",
    "# dataset_metaflux_final = []\n",
    "\n",
    "dataset_true_final_data = []\n",
    "dataset_pred_final_data = []\n",
    "# dataset_metaflux_final_data = []\n",
    "for data_value in data:\n",
    "#     print(data_value.shape)\n",
    "    data_value = np.expand_dims(data_value, axis=0)\n",
    "#     print(data_value.shape)\n",
    "    nodes, years, window, channels = data_value.shape\n",
    "#     print(nodes, years, window, channels)\n",
    "\n",
    "    dataset_true = unknown*np.ones((nodes, years, stride, len(output_channels)), dtype=np.float32)\n",
    "#     print(\"this is the dtat true shape\")\n",
    "#     print(dataset_true.shape)\n",
    "    dataset_pred = unknown*np.ones((nodes, years, stride, len(output_channels)), dtype=np.float32)\n",
    "#     dataset_metaflux = unknown*np.ones((nodes, years, stride, len(metaflux_channels)), dtype=np.float32)\n",
    "    for year in range(years):\n",
    "\n",
    "        #Get instance for each node\n",
    "        node_data = data_value[np.arange(nodes), year]\n",
    "        node_data = np.array(node_data, dtype=np.float32)\n",
    "#         print(\"this is node data shape\")\n",
    "#         print(node_data.shape)\n",
    "\n",
    "        for batch in range(math.ceil(nodes/batch_size)):\n",
    "\n",
    "            # GET BATCH DATA AND LABEL\n",
    "            batch_data = torch.from_numpy(node_data[batch*batch_size:(batch+1)*batch_size]).float().to(device)\n",
    "            batch_dynamic_input = batch_data[:, :, dynamic_channels].float().to(device)\n",
    "            batch_static_input = batch_data[:, :, static_channels].float().to(device)\n",
    "            batch_label = batch_data[:, :, output_channels].float().to(device)   #this is the true value\n",
    "#             batch_metaflux = batch_data[:, :, metaflux_channels].float().to(device)\n",
    "#             print(batch_dynamic_input.shape, batch_static_input.shape, batch_label.shape)\n",
    "            \n",
    "        \n",
    "            # GET OUTPUT\n",
    "            batch_pred = model(x_dynamic=batch_dynamic_input, x_static=batch_static_input)    #this is the pred value \n",
    "            print(\"this is batch pred shape\")\n",
    "            print(batch_pred.shape)\n",
    "            \n",
    "            #Slicing the batch_pred and batch_label into two halves and taking the second half.\n",
    "            half_idx = stride\n",
    "            batch_pred_second_half = batch_pred[:, -half_idx:]\n",
    "            batch_label_second_half = batch_label[:, -half_idx:]\n",
    "#             batch_metaflux_second_half = batch_metaflux[:, -half_idx:]\n",
    "#             print(\"this is the shape of the half one\")\n",
    "#             print(batch_label_second_half.shape)\n",
    "\n",
    "            # STORE OUTPUT\n",
    "            dataset_true[batch*batch_size:(batch+1)*batch_size, year] = batch_label_second_half.detach().cpu().numpy()  #this is the true value\n",
    "            dataset_pred[batch*batch_size:(batch+1)*batch_size, year] = batch_pred_second_half.detach().cpu().numpy()    #this is the predicted value\n",
    "#             dataset_metaflux[batch*batch_size:(batch+1)*batch_size, year] = batch_metaflux_second_half.detach().cpu().numpy()\n",
    "#     print(\"dtaset true\")\n",
    "#     print(dataset_true.shape)\n",
    "#     print(\"dtaset pred\")\n",
    "#     print(dataset_pred.shape)\n",
    "    dataset_true_final.append(dataset_true)\n",
    "    dataset_pred_final.append(dataset_pred)\n",
    "    \n",
    "# print(\"thos one\")\n",
    "# print(dataset_true_final[0].shape)\n",
    "# print(dataset_true_final[1].shape)\n",
    "#dataset_true_final = np.concatenate(dataset_true_final, axis=1)\n",
    "# print(\"THIS IS THE SHAPE OF THE DATASET TRUE FINAL\")\n",
    "# print(dataset_true_final.shape)\n",
    "\n",
    "#dataset_pred_final = np.concatenate(dataset_pred_final, axis=1)\n",
    "# print(\"THIS IS THE SHAPE OF THE DATASET PRED FINAL\")\n",
    "# print(dataset_pred_final.shape)\n",
    "\n",
    "# for true_final in dataset_true_final:\n",
    "#     dataset_true_final_data.append((true_final*dataset[\"train_data_stds\"][output_channels])+dataset[\"train_data_means\"][output_channels])\n",
    "    \n",
    "# for pred_final in dataset_pred_final:\n",
    "#     dataset_pred_final_data.append((pred_final*dataset[\"train_data_stds\"][output_channels])+dataset[\"train_data_means\"][output_channels])\n",
    "    \n",
    "# for metaflux_final in dataset_metaflux_final:\n",
    "#     dataset_metaflux_final_data.append((metaflux_final*dataset[\"train_data_stds\"][metaflux_channels])+dataset[\"train_data_means\"][metaflux_channels])\n",
    "# dataset_pred_final_data = np.array(dataset_pred_final_data)\n",
    "# print(\"shape1\")\n",
    "# print(dataset_pred_final_data.shape)\n",
    "# print(dataset_true_final_data.shape)\n",
    "# print(\"shape2\")\n",
    "# # print((UTILS.unstride_array(dataset_true_final)).shape)\n",
    "\n",
    "# print(dataset_true_final.shape)\n",
    "# print(dataset_pred_final.shape)\n",
    "\n",
    "# for value in dataset_true_final_data:\n",
    "#     print(\"one\")\n",
    "#     print(value.shape)\n",
    "#     dataset_true_final = unstride_array(value)\n",
    "#     print(\"two\")\n",
    "#     print(dataset_true_final.shape)\n",
    "final_output_true = []\n",
    "for value in dataset_true_final:\n",
    "#     print(value.shape)\n",
    "    value = np.reshape(value, (value.shape[0], value.shape[1] * value.shape[2], value.shape[3]))\n",
    "    print(value.shape)\n",
    "    final_output_true.append(value)\n",
    "    \n",
    "final_output_pred = []\n",
    "for value in dataset_pred_final:\n",
    "#     print(value.shape)\n",
    "    value = np.reshape(value, (value.shape[0], value.shape[1] * value.shape[2], value.shape[3]))\n",
    "    print(value.shape)\n",
    "    final_output_pred.append(value)\n",
    "    \n",
    "\n",
    "# per_sample_RMSE = UTILS.per_sample_RMSE(dataset_true_final, dataset_pred_final, unknown)\n",
    "# _, per_node_RMSE = UTILS.per_node_RMSE(dataset_true_final, dataset_pred_final, unknown)\n",
    "# per_sample_R2 = UTILS.per_sample_R2(dataset_true_final, dataset_pred_final, unknown)\n",
    "# _, per_node_R2 = UTILS.per_node_R2(dataset_true_final, dataset_pred_final, unknown)\n",
    "# print(\"Per Sample RMSE:{:.4f}\\tPer Node RMSE:{:.4f}\\tPer Sample R2:{:.4f}\\tPer Node R2:{:.4f}\".format(per_sample_RMSE, per_node_RMSE, per_sample_R2, per_node_R2))\n",
    "# with open(os.path.join(RESULT_DIR, \"{}_{}_{}\".format(file, index, \"ameriflux_true_{}\".format(fold))), 'wb') as f:\n",
    "#     pickle.dump(final_output_true, f)\n",
    "# with open(os.path.join(RESULT_DIR, \"{}_{}_{}_{}\".format(file, index, \"ameriflux_pred\", model_name)), 'wb') as f:\n",
    "#     pickle.dump(final_output_pred, f)\n",
    "    \n",
    "with open(os.path.join(RESULT_DIR, \"{}_{}_{}\".format(file, index, \"true_{}\".format(fold))), 'wb') as f:\n",
    "    pickle.dump(final_output_true, f)\n",
    "with open(os.path.join(RESULT_DIR, \"{}_{}_{}\".format(file, index, model_name)), 'wb') as f:\n",
    "    pickle.dump(final_output_pred, f)\n",
    "\n",
    "\n",
    "# np.save(os.path.join(RESULT_DIR, \"{}_{}_{}\".format(file, index, \"ameriflux_true_{}\".format(fold))), dataset_true_final)\n",
    "# np.save(os.path.join(RESULT_DIR, \"{}_{}_{}_{}\".format(file, index,\"ameriflux_pred\", model_name)), dataset_pred_final)\n",
    "# np.save(os.path.join(RESULT_DIR, \"{}_{}_{}\".format(file, index, \"metaflux_{}\".format(fold))), dataset_metaflux_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plain-turning",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_a100",
   "language": "python",
   "name": "main_a100"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
